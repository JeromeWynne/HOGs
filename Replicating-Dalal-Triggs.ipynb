{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Dalal & Triggs Paper 'Histogram of Oriented Gradients for Human Detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "\n",
    "- Histogram of Oriented Gradients\n",
    "    - Fine-scale gradients\n",
    "    - Fine orientation binning\n",
    "    - Coarse orientation binning\n",
    "    - High-quality local contrast normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear SVM is baseline classifier\n",
    "- MIT pedestrian test set\n",
    "\n",
    "\n",
    "1. Introduction\n",
    "2. Previous work\n",
    "3. Method overview\n",
    "4. Describe dataset\n",
    "5. Detailed description of process\n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant papers on human detection:\n",
    "- Polynomial SVM w/ rectified Haar wavelets\n",
    "- Edge-based classification\n",
    "- AdaBoost on Haar-like wavelets (Viola-Jones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Method overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Based on evaluating normalized local histograms of image gradient orientations in a dense grid\"\n",
    "- Divide image widow into cells, compute a 1D histogram of gradient directions for each cell\n",
    "- Invariance to illumination demands normalization\n",
    "- Normalization is done over groups of cells, called 'blocks'\n",
    "- Normalized descriptor blocks -> Histogram of Oriented Gradients descriptors\n",
    "- Tile the detection window with a dense grid of HOG descriptors. Use the combined feature vector.\n",
    "\n",
    "\n",
    "1. Input image\n",
    "2. Normalize gamma and colour\n",
    "3. Compute gradients across entire image\n",
    "4. Weighted vote into spatial and orientation cells\n",
    "5. Contrast normalize over blocks\n",
    "6. Collect HOGs over detection window\n",
    "7. Train Linear SVM against HOGs for people/no-people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Datasets and Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: MIT pedestrian database (509 training, 200 test images)\n",
    "Method:\n",
    "1239 positive training examples, reflected (2478 images).\n",
    "1218 person-free photos, from which 12180 patches were sampled (these patches constituted the negative samples).\n",
    "Train one detector for each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default detector's properties:\n",
    "    - RGB with no gamma correction\n",
    "    - [-1, 0, 1] gradients filter, no smoothing\n",
    "    - Linear gradient voting into 9 orientation bins in 0-180deg\n",
    "    - 16x16 pixel blocks\n",
    "    - Each pixel block contains 4 8x8 cells\n",
    "    - Gaussian spatial window with sigma=8 (weighting of hists)\n",
    "    - L2-Hys block normalization (contrast?)\n",
    "    - Block stride of 8 pixels\n",
    "    - 64 x 128 detection window (i.e. cropped person size)\n",
    "    - Linear SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient computation:\n",
    "    - [-1, 0, 1] masks, one for x, one for y\n",
    "\n",
    "Spatial/Orientation binning:\n",
    "    - Each pixel calculates a gradient-magnitude-weighted vote for edge orientation histogram channels.\n",
    "    - The votes are accumulated into orientation bins over a cell\n",
    "    - 0-180deg orientation bins\n",
    "    - Histogram votes are interpolated bilinearly between nearest bins for both orientation and magnitude\n",
    "    - Fine orientation binning (up to 9 bins)\n",
    "    \n",
    "Normalization and descriptor blocks:\n",
    "    - Local contrast normalization is essential\n",
    "    - Group cells into blocks, normalize each block separately\n",
    "    - Blocks should overlap\n",
    "    - 3x3 cell blocks, 6x6 pixel cells\n",
    "    - Downweighted block edges using a Gaussian spatial window\n",
    "    - Contast normalization was done using L2-norm \n",
    "    - Comments that Linear SVM depends on silhouette contours as cue, as opposed to internal conto\n",
    "        - Their SVM weighted the head and the gap between the legs most heavily.\n",
    "    - Context was important (they included a 16px min. border around people - removing this degraded performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating their results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of Oriented Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a single-channel image (i.e. grayscale)\n",
    "2. Compute x, y, gradients for entire image.\n",
    "    - Get L2 norm (i.e. gradient magnitude)\n",
    "    - Get orientation \n",
    "3. Calculate a magnitude-weighted vote for an edge orientation at each pixel.\n",
    "4. Accumulate these votes into a histogram for a cell (6x6 px). The histogram has 9 bins between 0 an 180 degrees (orientation is unsigned). Distribute votes from pixels whose orientations aren't in bin centers proportionally between adjacent bins.\n",
    "7. The final descriptor is a vector of each cell's histogram components from all blocks in the detection window (the detection window being the 64x128 image in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import imread\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieved from http://cbcl.mit.edu/software-datasets/PedestrianData.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ppm_to_jpg():\n",
    "    ppm_fps = glob.glob('./data/ppm/*')\n",
    "    for fp in ppm_fps:\n",
    "        img_name = fp.split('\\\\')[-1][:-4]\n",
    "        with Image.open(fp) as img:\n",
    "            img.save('./data/jpg/%s.jpg' % img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_fps = glob.glob('./data/jpg/*')\n",
    "pos_imgs = np.array([imread(img_fp) for img_fp in pos_fps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 128, 64, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_imgs.shape # All images are 64x128 (w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampled from negative training samples of http://pascal.inrialpes.fr/data/human/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_fps = glob.glob('./data/INRIAPerson/Train/neg/*')\n",
    "neg_imgs = np.array([imread(img_fp) for img_fp in neg_fps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract 64x128 subsamples from these images (~ 10k - 15k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_imgs[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
